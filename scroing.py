# -*- coding: utf-8 -*-
"""scroing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1swhhrMR3BrI8vSNFVkVfgIy2g858-Oqd
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report
import joblib

# Step 1: Load the dataset
def load_data(file_path):
    try:
        data = pd.read_csv(file_path)
        print("Data loaded successfully.")
        return data
    except Exception as e:
        print(f"Error loading data: {e}")
        return None

# Step 2: Preprocess the data
def preprocess_data(data, target_column):
    # Handle missing values
    data = data.dropna()

    # Encode categorical variables
    data = pd.get_dummies(data, drop_first=True)

    # Split features and target
    X = data.drop(target_column, axis=1)
    y = data[target_column]

    # Split into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Standardize the features
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    return X_train, X_test, y_train, y_test, scaler

# Step 3: Train the logistic regression model
def train_model(X_train, y_train):
    model = LogisticRegression()
    model.fit(X_train, y_train)
    print("Model training completed.")
    return model

# Step 4: Evaluate the model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    y_pred_prob = model.predict_proba(X_test)[:, 1]

    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("Recall:", recall_score(y_test, y_pred))
    print("F1 Score:", f1_score(y_test, y_pred))
    print("ROC AUC Score:", roc_auc_score(y_test, y_pred_prob))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))

# Step 5: Save the model and scaler
def save_model(model, scaler, model_path, scaler_path):
    joblib.dump(model, model_path)
    joblib.dump(scaler, scaler_path)
    print(f"Model saved to {model_path}")
    print(f"Scaler saved to {scaler_path}")

# Main function
def main():
    # File path to your dataset
    file_path = "credit_data.csv"  # Replace with your file path
    target_column = "default"  # Replace with your target column name

    data = load_data(file_path)
    if data is not None:
        X_train, X_test, y_train, y_test, scaler = preprocess_data(data, target_column)
        model = train_model(X_train, y_train)
        evaluate_model(model, X_test, y_test)
        save_model(model, scaler, "credit_scoring_model.pkl", "scaler.pkl")

if __name__ == "__main__":
    main()

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import confusion_matrix, classification_report
import joblib
import os

# Step 1: Generate synthetic data (if no file provided)
def generate_synthetic_data(file_path):
    if not os.path.exists(file_path):
        print("Dataset not found. Generating synthetic dataset...")
        np.random.seed(42)
        num_samples = 1000

        data = {
            'age': np.random.randint(18, 70, num_samples),
            'income': np.random.randint(20000, 150000, num_samples),
            'loan_amount': np.random.randint(1000, 50000, num_samples),
            'credit_score': np.random.randint(300, 850, num_samples),
            'employment_years': np.random.randint(0, 30, num_samples),
            'default': np.random.choice([0, 1], num_samples, p=[0.7, 0.3])
        }

        df = pd.DataFrame(data)
        df.to_csv(file_path, index=False)
        print(f"Synthetic dataset saved to {file_path}")

# Step 2: Load the dataset
def load_data(file_path):
    generate_synthetic_data(file_path)
    try:
        data = pd.read_csv(file_path)
        print("Data loaded successfully.")
        return data
    except Exception as e:
        print(f"Error loading data: {e}")
        return None

# Step 3: Preprocess the data
def preprocess_data(data, target_column):
    # Handle missing values
    data = data.dropna()

    # Encode categorical variables
    data = pd.get_dummies(data, drop_first=True)

    # Split features and target
    X = data.drop(target_column, axis=1)
    y = data[target_column]

    # Split into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Standardize the features
    scaler = StandardScaler()
    X_train = scaler.fit_transform(X_train)
    X_test = scaler.transform(X_test)

    return X_train, X_test, y_train, y_test, scaler

# Step 4: Train the logistic regression model
def train_model(X_train, y_train):
    model = LogisticRegression()
    model.fit(X_train, y_train)
    print("Model training completed.")
    return model

# Step 5: Evaluate the model
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    y_pred_prob = model.predict_proba(X_test)[:, 1]

    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("Recall:", recall_score(y_test, y_pred))
    print("F1 Score:", f1_score(y_test, y_pred))
    print("ROC AUC Score:", roc_auc_score(y_test, y_pred_prob))
    print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
    print("Classification Report:\n", classification_report(y_test, y_pred))

# Step 6: Save the model and scaler
def save_model(model, scaler, model_path, scaler_path):
    joblib.dump(model, model_path)
    joblib.dump(scaler, scaler_path)
    print(f"Model saved to {model_path}")
    print(f"Scaler saved to {scaler_path}")

# Main function
def main():
    # File path to your dataset
    file_path = "credit_data.csv"  # Replace with your file path
    target_column = "default"  # Replace with your target column name

    data = load_data(file_path)
    if data is not None:
        X_train, X_test, y_train, y_test, scaler = preprocess_data(data, target_column)
        model = train_model(X_train, y_train)
        evaluate_model(model, X_test, y_test)
        save_model(model, scaler, "credit_scoring_model.pkl", "scaler.pkl")

if __name__ == "__main__":
    main()